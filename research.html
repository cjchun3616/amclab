<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AMCL Homepage</title>

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet" 
    integrity="sha384-GLhlTQ8iRABdZLl6O3oVMWSktQOp6b7In1Zl3/Jr59b6EGGoI1aFkw7cmDA6j6gD" crossorigin="anonymous">    
    <link rel="stylesheet" href="style.css">
</head>
<body>

    <nav class="navbar navbar-expand-md bg-dark navbar-dark fixed-top">
        <div class="container">
          <a href="index" class="navbar-brand text-white">
            <div class="container">
                <img src="images/logo2.png" alt="로고" style="height: 40px;" class="logoImg">
            </div>
          </a>
  
          <button class="navbar-toggler" type="button" 
            data-bs-toggle="collapse" 
            data-bs-target="#nav-menu" 
            aria-controls="nav-menu" 
            aria-expanded="false" 
            aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
          </button>
  
          <div class="collapse navbar-collapse" id="nav-menu">
            <ul class="navbar-nav ms-auto my-2">
              <li class="nav-item"><a class="nav-link text-white fw-bold" href="index">Home</a></li>
              <li class="nav-item"><a class="nav-link text-white fw-bold" href="people">People</a></li>
              <li class="nav-item"><a class="nav-link text-white fw-bold" href="research">Research</a></li>
              <li class="nav-item"><a class="nav-link text-white fw-bold" href="publication">Publication</a></li>
              <li class="nav-item"><a class="nav-link text-white fw-bold" href="news">News</a></li>              
            </ul>
          </div>
        </div>
    </nav>

    <section class="mt-5 py-5 px-1">
      <!-- Voice Conversion (Perturbation AutoVC) -->
      <div class="container border-bottom border-2 border-dark px-1 pb-5 mb-5">
        <h2 class="text-dark fw-bold pb-2 text-h2-responsive">Zero-shot Voice Conversion</h2>            
        <div class="card research-hover mx-5">
            <div class="row align-items-center">
                <div class="col-10 mx-auto">
                    <img src="images/perturbation_autovc.png" class="card-img-top" alt="perturbation_autovc">
                </div>

                <div class="col-12">
                    <div class="card-body">
                        <h5 class="card-title fw-bold  text-h5-responsive">기술의 개요</h5>
                        <ul>
                          <li class="card-text text-p-responsive py-1 myText">
                            음색 변환(voice conversion) 기술은 음성에서 발화의 내용과 같은 화자(소스) 독립적인 언어적 정보는 유지한 채, 음색과 같은 화자(타겟) 종속적인 비언어적 정보를 원하는 정보로 바꾸는 기술을 의미합니다.
                            음성 합성(speech synthesis) 시스템이나, 발성 장애를 가진 개인을 위한 음성 보조기, 음성 데이터 생성과 같은 응용분야 중 개인적 특성을 분영하는 목적으로 활용 가능합니다.
                          </li>
                          <li class="card-text text-p-responsive py-1 myText">
                            초기의 음색 변환 기술들은 일반적으로 훈련 데이터로 병렬 데이터(parallel data)를 사용하였습니다. 
                            여기서, 병렬 데이터란 원 화자와 특정 화자의 음성 데이터가 동일한 언어적 정보가 쌍으로 구성된 것을 가리킵니다. 
                            이러한 병렬 데이터는 현실적으로 많은 양의 데이터를 모으는 것에는 한계가 있기 때문에, 현재는 비병렬 데이터(non-parallel data)를 활용한 음색 변환 기술이 많이 연구되고 있습니다.
                          </li>
                          <li class="card-text text-p-responsive py-1 myText">
                            오토인코더(autoencoder)모델 구조를 활용한 음색변환 방법의 경우, 은닉층(bottleneck) 차원을 조절하면서 음성 정보에 포함된 언어정보와 화자 정보를 분리합니다. 그러나 이러한 방법은 은닉층 차원에 따라 모델의 성능이 매우 민감하게 좌우 된다는 단점이 있습니다.
                            예를 들어, 은닉층 차원이 매우 좁을 경우 음성에 존재하는 화자 정보를 제거할 뿐만 아니라 언어정보까지 일부 손상될 수 있으며, 손상된 언어정보를 이용하여 음성을 복원할 경우 음성의 품질이 저하될 수 있습니다. 반대로 은닉층 차원이 매우 넓을 경우 음성에 언어정보와 화자정보를 제대로 분리할 수 없어, 복원된 음성의 화자 유사도가 낮아질 수 있습니다.
                          </li>
                          <li class="card-text text-p-responsive py-1 myText">
                            이러한 단점을 해결하기 위해, 정보 교란 방법(information perturbation)을 적용할 수 있습니다. 정보 교란 방법이란 음성에 존재하는 화자 정보를 왜곡시키는 방법입니다. 이러한 방법을 오토인코더 모델 구조에 적용할 경우, 은닉층 차원에 따른 정보 분리 방법을 사용하지 않을 수 있으며, 정보 교란 방법으로 언어정보는 손상되지 않기 때문에 음성의 품질과 화자 유사도 간의 trade-off 관계를 해결할 수 있습니다.
                            현재 다양한 생성 모델 기반의 음색 변환 기술들이 연구되고 있으며, 대표적인 방법으로는 diffusion 모델들을 활용한 음색 변환 기술들이 연구되고 있습니다.                            
                          </li>
                        </ul>  
                        <a href="https://www.amclab.kr/demo/perturbation_autovc/" target="_blank"><i class="bi bi-link-45deg text-primary"></i> 관련 데모 페이지 바로가기</a>
                        <br/>
                        <a href="https://github.com/cjchun3616/perturbation_autovc/" target="_blank"><i class="bi bi-link-45deg text-primary"></i> 관련 github 페이지 바로가기</a>
                    </div>
                </div>
            </div>                
            
        </div>
      </div>       

      <!-- Zero-shot TTS (GradTTS - Old) -->
      <!-- <div class="container border-bottom border-2 border-dark px-1 pb-5 mb-5">
        <h2 class="text-dark fw-bold pb-2 text-h2-responsive">Zero-shot Speech Synthesis (Text-to-Speech)</h2>
        <div class="card research-hover mx-5">
            <div class="row align-items-center">
                <div class="col-md-5">
                    <img src="images/zeroshot_gradtts.png" class="card-img-top" alt="gradtts1">
                </div>

                <div class="col-md-7">
                    <div class="card-body">
                        <h5 class="card-title fw-bold text-h5-responsive">기술의 개요</h5>
                        <ul>
                          <li class="card-text text-p-responsive py-1 myText">
                            음성 합성이란 자연어를 입력으로 받아 음성을 생성하는 기술을 의미합니다. 신경망 기반의 음성 합성 기술은 크게 2단계로 나누어져 있습니다. 
                            자연어를 음향 특징 벡터로 생성하는 음향 모델과 음향 특징 벡터를 음성 파형으로 변환하는 보코더 모델이 있습니다. 
                            우리는 특히, 자연어를 음향 특징 벡터로 생성하는 음향 모델을 주로 연구하고 있습니다.
                          </li>
                          <li class="card-text text-p-responsive py-1 myText">
                            기존의 음성 합성 모델은 단일 화자에 대해서 자연스러운 억양과 정확한 발음을 발성하는 음성을 생성하는데 주목하였습니다. 
                            하지만 최근에 연구되고 있는 음성 합성 모델은 단일 화자에 대해서 높은 음성 합성 성능을 보일 뿐만 아니라 다수의 화자에 대해서 음성 합성이 가능한 다화자(Multi-Speaker) 음성 합성 모델이 연구되고 있습니다. 
                            더 나아가 학습 중 보지 못한 화자(Unseen Speaker)에 대해서 음성을 합성할 수 있는 Zero-Shot Multi-Speaker 음성 합성 모델이 많이 연구되고 있습니다.
                          </li>
                          <li class="card-text text-p-responsive py-1 myText">
                            우리는 다양한 분야에서 높은 성능을 보이는 Diffusion 모델을 바탕으로 Zero-Shot Multi-Speaker 음성 합성 모델에 대해서 연구하고 있습니다. 
                            Diffusion 모델은 확산의 개념을 이용하여 데이터를 생성하는 방법으로 크게 2가지 단계로 나뉩니다. 
                            데이터의 분포에 점진적으로 노이즈를 주어 다루기 쉬운 데이터 분포로 변환하는 Forward Process와 다시 원본 데이터로 복원시키는 Backward Process를 통해 데이터를 생성하게 됩니다. 
                            이러한 Diffusion 기반의 음성 합성 모델 중 가장 성공적인 모델로는 Grad-TTS가 있습니다.
                          </li>
                          <li class="card-text text-p-responsive py-1 myText">
                            Grad-TTS는 트랜스포머(Transformer) 기반의 텍스트 인코더와 Diffusion 기반의 디코더로 구성된 음성 합성 모델로, 현재 단일 화자에 대해서 높은 음성 합성 성능을 보이는 모델입니다. 
                            우리는 이러한 Grad-TTS를 개선하여 Zero-Shot Multi-Speaker 환경에서 높은 성능을 보이는 음성 합성 모델을 개발하고 있습니다. 
                            또한, 다양한 Diffusion 모델들을 활용한 음성 합성 기술들을 연구하고 있습니다.
                          </li>
                        </ul> 

                        
                    </div>
                </div>
            </div>                
            
        </div>
      </div>     -->

      <!-- Zero-shot TTS (GradTTS - New) -->
      <div class="container border-bottom border-2 border-dark px-1 pb-5 mb-5">
        <h2 class="text-dark fw-bold pb-2 text-h2-responsive">Zero-shot Speech Synthesis (Text-to-Speech)</h2>
        <div class="card research-hover mx-5">
            <div class="row align-items-center">
                <!-- Image Column -->
                <div class="col-10 mx-auto">
                    <img src="images/zeroshot_gradtts.png" class="card-img-top" alt="gradtts2">
                </div>
    
                <!-- Description Column -->
                <div class="col-12">
                    <div class="card-body">
                        <!-- Content goes here -->
                        <h5 class="card-title fw-bold text-h5-responsive">기술의 개요</h5>
                        <ul>
                          <li class="card-text text-p-responsive py-1 myText">
                            음성 합성이란 자연어를 입력으로 받아 음성을 생성하는 기술을 의미합니다. 신경망 기반의 음성 합성 기술은 크게 2단계로 나누어져 있습니다. 
                            자연어를 음향 특징 벡터로 생성하는 음향 모델과 음향 특징 벡터를 음성 파형으로 변환하는 보코더 모델이 있습니다. 
                            우리는 특히, 자연어를 음향 특징 벡터로 생성하는 음향 모델을 주로 연구하고 있습니다.
                          </li>
                          <li class="card-text text-p-responsive py-1 myText">
                            기존의 음성 합성 모델은 단일 화자에 대해서 자연스러운 억양과 정확한 발음을 발성하는 음성을 생성하는데 주목하였습니다. 
                            하지만 최근에 연구되고 있는 음성 합성 모델은 단일 화자에 대해서 높은 음성 합성 성능을 보일 뿐만 아니라 다수의 화자에 대해서 음성 합성이 가능한 다화자(Multi-Speaker) 음성 합성 모델이 연구되고 있습니다. 
                            더 나아가 학습 중 보지 못한 화자(Unseen Speaker)에 대해서 음성을 합성할 수 있는 Zero-Shot Multi-Speaker 음성 합성 모델이 많이 연구되고 있습니다.
                          </li>
                          <li class="card-text text-p-responsive py-1 myText">
                            우리는 다양한 분야에서 높은 성능을 보이는 Diffusion 모델을 바탕으로 Zero-Shot Multi-Speaker 음성 합성 모델에 대해서 연구하고 있습니다. 
                            Diffusion 모델은 확산의 개념을 이용하여 데이터를 생성하는 방법으로 크게 2가지 단계로 나뉩니다. 
                            데이터의 분포에 점진적으로 노이즈를 주어 다루기 쉬운 데이터 분포로 변환하는 Forward Process와 다시 원본 데이터로 복원시키는 Backward Process를 통해 데이터를 생성하게 됩니다. 
                            이러한 Diffusion 기반의 음성 합성 모델 중 가장 성공적인 모델로는 Grad-TTS가 있습니다.
                          </li>
                          <li class="card-text text-p-responsive py-1 myText">
                            Grad-TTS는 트랜스포머(Transformer) 기반의 텍스트 인코더와 Diffusion 기반의 디코더로 구성된 음성 합성 모델로, 현재 단일 화자에 대해서 높은 음성 합성 성능을 보이는 모델입니다. 
                            우리는 이러한 Grad-TTS를 개선하여 Zero-Shot Multi-Speaker 환경에서 높은 성능을 보이는 음성 합성 모델을 개발하고 있습니다. 
                            또한, 다양한 Diffusion 모델들을 활용한 음성 합성 기술들을 연구하고 있습니다.
                          </li>
                        </ul> 
                        <a href="https://www.amclab.kr/demo/zero_shot_gradtts/" target="_blank"><i class="bi bi-link-45deg text-primary"></i> 관련 데모 페이지 바로가기</a>
                    </div>
                </div>
            </div>                
        </div>
      </div>    

      <!-- Speaker Recognition -->
      <div class="container border-bottom border-2 border-dark px-1 pb-5 mb-5">
        <h2 class="text-dark fw-bold pb-2 text-h2-responsive">Speaker Recognition</h2>
        <div class="card research-hover mx-5">
            <div class="row align-items-center">
                <!-- Image Column -->
                <div class="col-10 mx-auto">
                    <img src="images/cssp.png" class="card-img-top" alt="speaker_recognition">
                </div>
    
                <!-- Description Column -->
                <div class="col-12">
                    <div class="card-body">
                        <!-- Content goes here -->
                        <h5 class="card-title fw-bold text-h5-responsive">기술의 개요</h5>
                        <ul>
                          <li class="card-text text-p-responsive py-1 myText">
                            화자인식이란 음성 신호로 부터 화자를 식별할 수 있는 특징을 추출하여 입력 발화가 어느 화자의 발화인지 판별하는 기술을 의미합니다. 
                            화자인식은 입력된 발화가 등록된 화자 중 어느 화자와 가장 유사한지 판단하는 화자 식별 기술과 두 쌍의 발화가 같은 화자의 발화인지 판단하는 화자 검증 기술로 구성됩니다. 
                          </li>
                          <li class="card-text text-p-responsive py-1 myText">
                            기존의 화자인식은 딥러닝 모델을 cross-entropy, AMSoftmax, AAMSoftmax등와 같은 손실함수을 통해 화자를 식별하는 분류 접근 방식과 contrastive loss, triplet loss와 같은 손실함수를 통해 representation 간의 관계을 학습하는 대조학습 접근 방식 등의 방법이 존재합니다. 
                            이러한 방식 중에서 우리는 대조학습을 활용한 화자인식 연구를 하고 있습니다.
                          </li>
                          <li class="card-text text-p-responsive py-1 myText">
                            최근 널리 활용되는 대조학습 기법 중에는 대량의 데이터셋에 대해 InfoNCE와 같은 손실함수를 활용하여 contrastive lanaguage image pretraining (CLIP)와 같이 one-to-many간의 representation간의 관계를 학습하는 연구들이 많이 제안되고 있습니다. 
                            이러한 기법이 zero-shot classfication에서 state-of-the-art을 달성하는등 높은 성능을 달성하고 있다는 점에서, 우리는 CLIP의 학습 기법을 기반으로 voxceleb dataset을 활용해 강건한 화자인식 모델을 학습하는 프레임워크를 개발하는 연구를 수행합니다.
                          </li>
                          <li class="card-text text-p-responsive py-1 myText">
                            이와 같이 화자인식은 다양한 딥러닝 모델 및 학습기법을 응용한다면, 더욱 강건한 모델을 학습할 수 있습니다. 
                            우리는 실제로는 다른 화자의 발성이지만 유사한 특징을 가지고 있어 오인하기 쉬운 하드 네거티브 샘플에 대응하기 위한 하드 네거티브 샘플링 기법을 활용하는 등 다양한 화자인식 연구를 수행합니다.
                          </li>
                        </ul> 
                        <a href="https://github.com/CJchanghwan/speaker_recognition_framework/" target="_blank"><i class="bi bi-link-45deg text-primary"></i> 관련 github 페이지 바로가기</a>
                    </div>
                </div>
            </div>                
        </div>
      </div>    
      

      <!-- Machine Translation (Old) -->
      <!-- <div class="container border-bottom border-2 border-dark px-1 pb-5 mb-5">
        <h2 class="text-dark fw-bold pb-2 text-h2-responsive">Machine Translation (Math Word Problem)</h2>
        <div class="card research-hover mx-5">
            <div class="row align-items-center">
                <div class="col-md-5">
                    <img src="images/conformer.png" class="card-img-top" alt="conformer">
                </div>

                <div class="col-md-7">
                    <div class="card-body">
                        <h5 class="card-title fw-bold text-h5-responsive">기술의 개요</h5>
                        <ul>
                            <li class="card-text text-p-responsive py-1 myText">
                              문장형 수학문제(Math Word Problem, MWP)는 수식과 같은 수학적 언어가 아닌 자연어로 서술된 수학 문제를 의미합니다.
                              예를 들어서, "사과가 5개가 있었는데, 2개를 먹었다면 몇 개가 남았을까?"와 같이 문장으로 구성된 수학문제를 제시하고, 
                              인공지능 모델은 단순하게 정답만을 추론하는 것이 아니라, "5 - 2"와 같은 풀이식을 도출합니다.
                            </li>
                            <li class="card-text text-p-responsive py-1 myText">
                              주어진 수학문제를 풀어내는 단순 계산문제와 달리 문장형 수학문제에서는 주어진 상황을 이해하고, 이를 토대로 대응하는 풀이식을 세워야 합니다.
                              이로 인하여 문장형 수학문제 풀이모델은 문제 해결력에 더불어 독해력과 추론력이 요구된다고 볼 수 있습니다.
                            </li>
                            <li class="card-text text-p-responsive py-1 myText">
                              우리는 기계 번역(machine translation) 구조 기반의 문장형 수학문제 풀이가 가능한 인공지능 모델을 연구하고 있습니다.
                              다양한 기계 번역 구조들이 연구되고 있지만, 
                              현재 많이 활용되고 좋은 성능을 보이고 있는 트랜스포머(Transformer) 및 컨포머(Conforemer) 기반으로 문장형 수학문제 풀이 모델을 연구하고 있습니다.                              
                            </li>
                            <li class="card-text text-p-responsive py-1 myText">
                              Conformer는 입력 데이터의 지역적 정보와 전역적 정보를 유기적으로 활용하기 위해 합성곱 신경망과 Transformer를 결합한 모델이라고 볼 수 있습니다. 
                              현재 Conformer 모델은 음성 인식과 화자 분리와 같은 음성 분야에서 Transformer보다 뛰어난 성능을 보이며 활발히 연구되고 있습니다.                              
                            </li>

                            <li class="card-text text-p-responsive py-1 myText">
                              Conformer의 데이터 특징 정보 활용 방식은 자연어 처리에서도 문장 내 단어 간의 대응 관계 및 특징 학습에 효과를 보일 수 있을 것으로 예상되며, 
                              우리는 이러한 Conformer 모델 기반의 문장형 수학문제 풀이 모델을 개발하고 있습니다.
                            </li>
                        </ul>                            

                        <p class="text-muted text-p-responsive">*그림 출처 : 콘포머 기반 한국어 음성인식 논문</p>
                    </div>
                </div>
            </div>                
            
        </div>
      </div>  -->
      
      


  </section>







    <footer class="bg-dark text-white">
      <div class="container text-center">
        <p class="py-4 mb-0">Copyright 2023 Designed by <a href="index" class="link-warning text-decoration-none">Advanced Multimedia Computing Lab.</a></p>
      </div>      

    </footer>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js" 
    integrity="sha384-w76AqPfDkMBDXo30jS1Sgez6pr3x5MlQ1ZAGC+nuZB+EYdgRZgiwxhTBTkF7CXvN" crossorigin="anonymous"></script>
</body>